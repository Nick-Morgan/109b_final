{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be trying a number of different models, we decided to create a function that will store all of the outcomes of interest in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, predictors, response, cv=False, params=None):\n",
    "    \"\"\"\n",
    "    evaluate_model()\n",
    "    \n",
    "    -splits the predictors & response variables into train and test sets. \n",
    "    -creates a dictionary of model outcomes that are of interest\n",
    "    -if specified, this function will use cross-validation to determine the optimal parameters for a given model\n",
    "    \n",
    "    inputs:\n",
    "        -model: a model object to be fitted\n",
    "        -predictors: an array, series, or dataframe of predictor variable(s)\n",
    "        -response: an array or series of the response variable\n",
    "        -cv: whether or not to cross-validate the model's parameters (default=False)\n",
    "        -params: if cv=True, params are required to indicate what parameters to optimize in the given model (default=None)\n",
    "        \n",
    "    outputs:\n",
    "        -a results dictionary containing the following:\n",
    "            -a fitted model object\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    train_x, test_x = train_test_split(predictors, test_size=0.5, random_state=9001)\n",
    "    train_y, test_y = train_test_split(response, test_size=0.5, random_state=9001)\n",
    "    \n",
    "    if cv:\n",
    "        model = GridSearchCV(model, params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    \n",
    "    classif = OneVsRestClassifier(model)\n",
    "    classif.fit(train_x, train_y)\n",
    "    \n",
    "    train_yhat = classif.predict(train_x)\n",
    "    test_yhat = classif.predict(test_x)\n",
    "    \n",
    "    results['fitted_model'] = classif\n",
    "    \n",
    "    results['train_yhat'] = train_yhat\n",
    "    results['test_yhat'] = test_yhat\n",
    "    \n",
    "    train_y_score = classif.decision_function(train_x)\n",
    "    test_y_score = classif.decision_function(test_x)\n",
    "    \n",
    "    results['train_average_precision'] = average_precision_score(train_y, train_y_score)\n",
    "    results['test_average_precision'] = average_precision_score(test_y, test_y_score)\n",
    "    \n",
    "    results['train_classification_report'] = classification_report(train_y, train_yhat)\n",
    "    results['test_classification_report'] = classification_report(test_y, test_yhat)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words -TMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in previously saved ndarrays\n",
    "binary_tmdb = np.load('data/binary_tmdb.npy') #response\n",
    "binary_imdb = np.load('data/binary_imdb.npy') #response\n",
    "\n",
    "tmdb_bow = np.load('data/tmdb_bow.npy') #predictor\n",
    "imdb_bow = np.load('data/tmdb_bow.npy') #predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tmdb_bow_svc = evaluate_model(model=SVC(class_weight='balanced'), predictors=tmdb_bow, \n",
    "                                   response=binary_tmdb, cv=True, \n",
    "                                   params={'kernel':['linear'], 'C':[0.01, 0.1, 1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        88\n",
      "          1       0.92      1.00      0.96        55\n",
      "          2       0.95      1.00      0.97        52\n",
      "          3       0.99      0.97      0.98       327\n",
      "          4       1.00      1.00      1.00        26\n",
      "          5       0.17      1.00      0.29        84\n",
      "          6       0.87      1.00      0.93       121\n",
      "          7       0.08      1.00      0.14        39\n",
      "          8       1.00      1.00      1.00        11\n",
      "          9       0.92      1.00      0.96       112\n",
      "         10       0.19      1.00      0.31        93\n",
      "         11       0.93      1.00      0.96        52\n",
      "         12       0.93      1.00      0.96        55\n",
      "         13       1.00      1.00      1.00        24\n",
      "         14       0.00      0.00      0.00        77\n",
      "         15       0.97      1.00      0.98        59\n",
      "         16       0.95      1.00      0.97        35\n",
      "         17       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.76      0.93      0.80      1313\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.32      0.34        79\n",
      "          1       0.28      0.21      0.24        38\n",
      "          2       0.42      0.22      0.29        46\n",
      "          3       0.73      0.75      0.74       316\n",
      "          4       0.33      0.04      0.07        26\n",
      "          5       0.16      1.00      0.28        82\n",
      "          6       0.35      0.41      0.37       115\n",
      "          7       0.07      1.00      0.12        33\n",
      "          8       0.00      0.00      0.00        22\n",
      "          9       0.44      0.41      0.42       112\n",
      "         10       0.17      1.00      0.28        83\n",
      "         11       0.62      0.30      0.40        54\n",
      "         12       0.42      0.29      0.34        45\n",
      "         13       0.33      0.05      0.09        20\n",
      "         14       0.00      0.00      0.00        71\n",
      "         15       0.45      0.25      0.32        52\n",
      "         16       0.61      0.44      0.51        25\n",
      "         17       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.42      0.51      0.41      1222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(tmdb_bow_svc['train_classification_report'])\n",
    "print(tmdb_bow_svc['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that overfitting may be occuring. The training precision/recall is 0.76/0.93, respectively, whereas the test results are 0.42/0.51, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words -TMDB and IMDB Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_bow =  np.load('data/combined_bow.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "combined_bow_svc = evaluate_model(model=SVC(class_weight='balanced'), predictors=combined_bow, \n",
    "                                   response=binary_tmdb, cv=True, \n",
    "                                   params={'kernel':['linear'], 'C':[0.01, 0.1, 1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        88\n",
      "          1       1.00      1.00      1.00        55\n",
      "          2       0.98      1.00      0.99        52\n",
      "          3       1.00      0.99      1.00       327\n",
      "          4       0.96      1.00      0.98        26\n",
      "          5       1.00      1.00      1.00        84\n",
      "          6       0.96      1.00      0.98       121\n",
      "          7       0.08      1.00      0.14        39\n",
      "          8       1.00      1.00      1.00        11\n",
      "          9       0.97      1.00      0.98       112\n",
      "         10       0.96      1.00      0.98        93\n",
      "         11       0.96      1.00      0.98        52\n",
      "         12       0.98      1.00      0.99        55\n",
      "         13       0.05      1.00      0.09        24\n",
      "         14       0.00      0.00      0.00        77\n",
      "         15       1.00      1.00      1.00        59\n",
      "         16       0.92      0.94      0.93        35\n",
      "         17       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.88      0.94      0.89      1313\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.32      0.41        79\n",
      "          1       0.47      0.18      0.26        38\n",
      "          2       0.71      0.22      0.33        46\n",
      "          3       0.78      0.85      0.81       316\n",
      "          4       0.67      0.08      0.14        26\n",
      "          5       0.62      0.26      0.36        82\n",
      "          6       0.49      0.34      0.40       115\n",
      "          7       0.07      1.00      0.12        33\n",
      "          8       0.00      0.00      0.00        22\n",
      "          9       0.51      0.38      0.43       112\n",
      "         10       0.64      0.49      0.56        83\n",
      "         11       0.88      0.26      0.40        54\n",
      "         12       0.44      0.18      0.25        45\n",
      "         13       0.04      1.00      0.08        20\n",
      "         14       0.00      0.00      0.00        71\n",
      "         15       0.80      0.23      0.36        52\n",
      "         16       0.88      0.56      0.68        25\n",
      "         17       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.58      0.46      0.46      1222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(combined_bow_svc['train_classification_report'])\n",
    "print(combined_bow_svc['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Words2Vec\n",
    "\n",
    "One additional step is required before we can use w2v as a predictor variable. The binarizer variable is an array of lists, whereas w2v is an array of arrays. To maintain consistency, we will have to turn w2v into an array of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmdb_w2v = np.load('data/tmdb_w2v.npy')\n",
    "imdb_w2v = np.load('data/imdb_w2v.npy')\n",
    "\n",
    "tmdb_w2v = np.apply_along_axis(lambda x: list(x), 0, tmdb_w2v)\n",
    "imdb_w2v = np.apply_along_axis(lambda x: list(x), 0, tmdb_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tmdb_w2v_svc = evaluate_model(model=SVC(class_weight='balanced'), predictors=tmdb_w2v, \n",
    "                                   response=binary_tmdb, cv=True, \n",
    "                                   params={'kernel':['linear'], 'C':[0.01, 0.1, 1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.92      0.72        88\n",
      "          1       0.11      1.00      0.20        55\n",
      "          2       0.59      0.92      0.72        52\n",
      "          3       0.90      0.84      0.87       327\n",
      "          4       0.52      0.88      0.66        26\n",
      "          5       0.17      1.00      0.29        84\n",
      "          6       0.56      0.95      0.70       121\n",
      "          7       0.08      1.00      0.14        39\n",
      "          8       0.92      1.00      0.96        11\n",
      "          9       0.62      0.62      0.62       112\n",
      "         10       0.67      0.71      0.69        93\n",
      "         11       0.62      0.96      0.76        52\n",
      "         12       0.11      1.00      0.20        55\n",
      "         13       0.05      1.00      0.09        24\n",
      "         14       0.00      0.00      0.00        77\n",
      "         15       0.63      0.97      0.77        59\n",
      "         16       0.66      1.00      0.80        35\n",
      "         17       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.55      0.83      0.61      1313\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.68      0.54        79\n",
      "          1       0.08      1.00      0.14        38\n",
      "          2       0.47      0.72      0.57        46\n",
      "          3       0.78      0.77      0.78       316\n",
      "          4       0.27      0.46      0.34        26\n",
      "          5       0.16      1.00      0.28        82\n",
      "          6       0.40      0.73      0.52       115\n",
      "          7       0.07      1.00      0.12        33\n",
      "          8       0.67      0.27      0.39        22\n",
      "          9       0.50      0.46      0.48       112\n",
      "         10       0.60      0.70      0.65        83\n",
      "         11       0.45      0.61      0.52        54\n",
      "         12       0.09      1.00      0.17        45\n",
      "         13       0.04      1.00      0.08        20\n",
      "         14       0.00      0.00      0.00        71\n",
      "         15       0.53      0.77      0.63        52\n",
      "         16       0.50      0.88      0.64        25\n",
      "         17       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.46      0.70      0.50      1222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tmdb_w2v_svc['train_classification_report'])\n",
    "print(tmdb_w2v_svc['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_w2v_mean = np.load('data/combined_w2v_mean.npy')\n",
    "\n",
    "combined_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, combined_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "combined_w2v_mean_svc = evaluate_model(model=SVC(class_weight='balanced'), predictors=combined_w2v_mean, \n",
    "                                   response=binary_tmdb, cv=True, \n",
    "                                   params={'kernel':['linear'], 'C':[0.01, 0.1, 1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.91      0.74        88\n",
      "          1       0.11      1.00      0.20        55\n",
      "          2       0.53      0.94      0.68        52\n",
      "          3       0.89      0.87      0.88       327\n",
      "          4       0.48      1.00      0.65        26\n",
      "          5       0.45      0.73      0.55        84\n",
      "          6       0.57      0.88      0.69       121\n",
      "          7       0.08      1.00      0.14        39\n",
      "          8       1.00      1.00      1.00        11\n",
      "          9       0.61      0.90      0.73       112\n",
      "         10       0.66      0.90      0.76        93\n",
      "         11       0.59      0.96      0.73        52\n",
      "         12       0.64      0.65      0.65        55\n",
      "         13       0.05      1.00      0.09        24\n",
      "         14       0.00      0.00      0.00        77\n",
      "         15       0.63      0.97      0.77        59\n",
      "         16       0.62      0.89      0.73        35\n",
      "         17       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.59      0.84      0.66      1313\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.75      0.58        79\n",
      "          1       0.08      1.00      0.14        38\n",
      "          2       0.45      0.87      0.60        46\n",
      "          3       0.80      0.85      0.83       316\n",
      "          4       0.25      0.38      0.30        26\n",
      "          5       0.49      0.68      0.57        82\n",
      "          6       0.41      0.74      0.53       115\n",
      "          7       0.07      1.00      0.12        33\n",
      "          8       0.75      0.55      0.63        22\n",
      "          9       0.43      0.62      0.51       112\n",
      "         10       0.49      0.77      0.60        83\n",
      "         11       0.43      0.67      0.53        54\n",
      "         12       0.33      0.31      0.32        45\n",
      "         13       0.04      1.00      0.08        20\n",
      "         14       0.00      0.00      0.00        71\n",
      "         15       0.62      0.88      0.73        52\n",
      "         16       0.45      0.76      0.57        25\n",
      "         17       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.49      0.71      0.55      1222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(combined_w2v_mean_svc['train_classification_report'])\n",
    "print(combined_w2v_mean_svc['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined w2v matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_w2v_matrix = np.load('data/combined_w2v_matrix.npy')\n",
    "\n",
    "combined_w2v_matrix = np.apply_along_axis(lambda x: list(x), 0, combined_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "combined_w2v_matrix_svc = evaluate_model(model=SVC(class_weight='balanced'), predictors=combined_w2v_matrix, \n",
    "                                   response=binary_tmdb, cv=True, \n",
    "                                   params={'kernel':['linear'], 'C':[0.01, 0.1, 1.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.91      0.74        88\n",
      "          1       0.11      1.00      0.20        55\n",
      "          2       0.53      0.94      0.68        52\n",
      "          3       0.89      0.87      0.88       327\n",
      "          4       0.48      1.00      0.65        26\n",
      "          5       0.45      0.73      0.55        84\n",
      "          6       0.57      0.88      0.69       121\n",
      "          7       0.08      1.00      0.14        39\n",
      "          8       1.00      1.00      1.00        11\n",
      "          9       0.61      0.90      0.73       112\n",
      "         10       0.66      0.90      0.76        93\n",
      "         11       0.59      0.96      0.73        52\n",
      "         12       0.64      0.65      0.65        55\n",
      "         13       0.05      1.00      0.09        24\n",
      "         14       0.00      0.00      0.00        77\n",
      "         15       0.63      0.97      0.77        59\n",
      "         16       0.62      0.89      0.73        35\n",
      "         17       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.59      0.84      0.66      1313\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.75      0.58        79\n",
      "          1       0.08      1.00      0.14        38\n",
      "          2       0.45      0.87      0.60        46\n",
      "          3       0.80      0.85      0.83       316\n",
      "          4       0.25      0.38      0.30        26\n",
      "          5       0.49      0.68      0.57        82\n",
      "          6       0.41      0.74      0.53       115\n",
      "          7       0.07      1.00      0.12        33\n",
      "          8       0.75      0.55      0.63        22\n",
      "          9       0.43      0.62      0.51       112\n",
      "         10       0.49      0.77      0.60        83\n",
      "         11       0.43      0.67      0.53        54\n",
      "         12       0.33      0.31      0.32        45\n",
      "         13       0.04      1.00      0.08        20\n",
      "         14       0.00      0.00      0.00        71\n",
      "         15       0.62      0.88      0.73        52\n",
      "         16       0.45      0.76      0.57        25\n",
      "         17       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.49      0.71      0.55      1222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(combined_w2v_matrix_svc['train_classification_report'])\n",
    "print(combined_w2v_matrix_svc['test_classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
