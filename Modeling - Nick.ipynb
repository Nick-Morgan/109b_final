{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be trying a number of different models, we decided to create a function that will store all of the outcomes of interest in a dictionary. The function below does an 80/20 train/test split. We also fit all of our models using a 50/50 split, but had better accuracy when using 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, predictors, response, cv=False, params=None):\n",
    "    \"\"\"\n",
    "    evaluate_model()\n",
    "    \n",
    "    -splits the predictors & response variables into train and test sets. \n",
    "    -creates a dictionary of model outcomes that are of interest\n",
    "    -if specified, this function will use cross-validation to determine the optimal parameters for a given model\n",
    "    \n",
    "    inputs:\n",
    "        -model: a model object to be fitted\n",
    "        -predictors: an array, series, or dataframe of predictor variable(s)\n",
    "        -response: an array or series of the response variable\n",
    "        -cv: whether or not to cross-validate the model's parameters (default=False)\n",
    "        -params: if cv=True, params are required to indicate what parameters to optimize in the given model (default=None)\n",
    "        \n",
    "    outputs:\n",
    "        -a results dictionary containing the following:\n",
    "            -a fitted model object\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    train_x, test_x = train_test_split(predictors, test_size=0.2, random_state=9001)\n",
    "    train_y, test_y = train_test_split(response, test_size=0.2, random_state=9001)\n",
    "    \n",
    "    if cv:\n",
    "        model = GridSearchCV(model, params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    \n",
    "    classif = OneVsRestClassifier(model)\n",
    "    classif.fit(train_x, train_y)\n",
    "    \n",
    "    train_yhat = classif.predict(train_x)\n",
    "    test_yhat = classif.predict(test_x)\n",
    "    \n",
    "    results['fitted_model'] = classif\n",
    "    \n",
    "    results['train_yhat'] = train_yhat\n",
    "    results['test_yhat'] = test_yhat\n",
    "    \n",
    "    #train_y_score = classif.decision_function(train_x)\n",
    "    #test_y_score = classif.decision_function(test_x)\n",
    "    \n",
    "    #results['train_average_precision'] = average_precision_score(train_y, train_y_score)\n",
    "    #results['test_average_precision'] = average_precision_score(test_y, test_y_score)\n",
    "    \n",
    "    results['train_recall_score'] = recall_score(train_y, train_yhat, average='weighted')\n",
    "    results['test_recall_score'] = recall_score(test_y, test_yhat, average='weighted')\n",
    "    \n",
    "    results['train_precision_score'] = precision_score(train_y, train_yhat,average='weighted')\n",
    "    results['test_precision_score'] = precision_score(test_y, test_yhat,average='weighted')\n",
    "    \n",
    "    results['train_classification_report'] = classification_report(train_y, train_yhat,target_names=target_names)\n",
    "    results['test_classification_report'] = classification_report(test_y, test_yhat,target_names=target_names)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we created the [MultiLabel Binarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer) array in a previous notebook, it created 18 classes corresponding to the 18 genres found in our dataset. These classes were labeled as class 0 through class 17, sorted in numeric order of the genre id. In order to improve readability of our report, we will set the target_names to be the name of the genre, rather than the id. The target names need to be in the same order as the MultiLabel Binarizer, so we will do two steps:\n",
    "\n",
    "    1: Sort the keys of the id_to_genre dictionary in ascending numeric order\n",
    "    2: Use the sorted keys against the id_to_genre dictionary to create the ordered target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "id_to_genre = json.load(open('data/id_to_genre.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(int(key) for key in id_to_genre.keys())\n",
    "keys.sort()\n",
    "\n",
    "target_names = [id_to_genre[str(key)] for key in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load in our arrays to be used as predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmdb_bow = np.load('data/tmdb_bow.npy')\n",
    "imdb_bow = np.load('data/imdb_bow.npy')\n",
    "combined_bow = np.load('data/combined_bow.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Words2Vector arrays require 2 additional steps in order to be used as predictors:\n",
    "\n",
    "    1: They need to be converted to an array of lists (they are currently an array of arrays, which is incompatible with the structure of the response variable, which is also an array of lists).\n",
    "    2: The values need to be standardized between 0 and 1, because (todo - there was an error when they were negative. I will have to re-run and see what caused the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmdb_w2v_mean = np.load('data/tmdb_w2v_mean.npy')\n",
    "imdb_w2v_mean = np.load('data/imdb_w2v_mean.npy')\n",
    "combined_w2v_mean = np.load('data/combined_w2v_mean.npy')\n",
    "\n",
    "tmdb_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, tmdb_w2v_mean)\n",
    "imdb_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, imdb_w2v_mean)\n",
    "combined_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, combined_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "scale.fit(tmdb_w2v_mean)\n",
    "tmdb_w2v_mean = scale.transform(tmdb_w2v_mean)\n",
    "\n",
    "scale.fit(imdb_w2v_mean)\n",
    "imdb_w2v_mean = scale.transform(imdb_w2v_mean)\n",
    "\n",
    "scale.fit(combined_w2v_mean)\n",
    "combined_w2v_mean = scale.transform(combined_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_tmdb = np.load('data/binary_tmdb.npy')\n",
    "binary_imdb = np.load('data/binary_imdb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all of the parameters necessary to run our function. We will create models using 6 different predictors:\n",
    "\n",
    "    1: Bag of words using the TMDB plot\n",
    "    2: Bag of words using the IMDB plot\n",
    "    3: Bag of words using the combined plots from both sources\n",
    "    4: Words2Vectors using the TMDB plot\n",
    "    5: Words2Vectors using the IMDB plot\n",
    "    6: Words2Vectors using the combined plots from both sources\n",
    "    \n",
    "\n",
    "We will use these predictors to create 3 classification models to predict movie genres:\n",
    "\n",
    "    1: Naive-Bayes, with a cross-validated smoothing parameter\n",
    "    2: Stochastic Gradient Descent, with a cross-validated regularization multiplier.\n",
    "    3: Support Vector machines, with a cross-validated penalty parameter of the error term.\n",
    "    \n",
    "    \n",
    "This will result in 18 total models being created. We will store the results of each model in a dictionary, which will allow us to identify the best-performing models.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDict = {'Naive-Bayes':{'model':MultinomialNB(),\n",
    "                           'params':{'alpha':[0.01,0.1,1.0]}},\n",
    "            \n",
    "            'SGD':{'model':SGDClassifier(loss='hinge',penalty='l2',n_iter=5,random_state=9001),\n",
    "                   'params':{'alpha':[0.01,0.1,1.0]}},\n",
    "            \n",
    "            'SVC':{'model':SVC(class_weight='balanced', kernel='linear'),\n",
    "                   'params':{'C':[0.01,0.1,1.0]}}\n",
    "           }\n",
    "\n",
    "predictorDict = {\n",
    "                 'tmdb_bow':tmdb_bow,\n",
    "                 'imdb_bow':imdb_bow,\n",
    "                 'combined_bow':combined_bow,\n",
    "                 'tmdb_w2v_mean':tmdb_w2v_mean,\n",
    "                 'imdb_w2v_mean':imdb_w2v_mean,\n",
    "                 'combined_w2v_mean':combined_w2v_mean\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A706GZZ\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "resultsDict = {}\n",
    "for model in modelDict:\n",
    "    for predictor in predictorDict:\n",
    "        resultsDict['{0}-{1}'.format(model,predictor)] = evaluate_model(model = modelDict[model]['model'],\n",
    "                                                                        predictors = predictorDict[predictor], \n",
    "                                                                        response = binary_tmdb,\n",
    "                                                                        cv=True,\n",
    "                                                                        params=modelDict[model]['params'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This next cell will be removed when we submit our final report. It is just being used now to temporarily store the results of the models to avoid having to re-run the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hacky code to temporarily store results in a json file\n",
    "temp = resultsDict.copy()\n",
    "\n",
    "#hacky code to remove results that can't be stored in a json\n",
    "for key in temp:\n",
    "    for inner_key in temp[key]:\n",
    "        if inner_key not in ['train_recall_score','test_recall_score','train_precision_score','test_precision_score',\n",
    "                            'train_classification_report', 'test_classification_report']:\n",
    "            temp[key][inner_key] = None\n",
    "        \n",
    "        \n",
    "import json\n",
    "\n",
    "with open('data/resultsDict.json','w') as file:\n",
    "    json.dump(temp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make a dataframe of the scores for the sake of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = ['train_recall_score','test_recall_score',\n",
    "          'train_precision_score','test_precision_score']\n",
    "\n",
    "results_df = pd.DataFrame(resultsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = results_df.loc[results_df.index.isin(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = results_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_recall = results_df['test_recall_score'].idxmax()\n",
    "best_precision = results_df['test_precision_score'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC-combined_bow</th>\n",
       "      <td>0.646031</td>\n",
       "      <td>0.538136</td>\n",
       "      <td>0.959229</td>\n",
       "      <td>0.993214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_w2v_mean</th>\n",
       "      <td>0.571619</td>\n",
       "      <td>0.713983</td>\n",
       "      <td>0.714626</td>\n",
       "      <td>0.888027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      test_precision_score test_recall_score  \\\n",
       "SVC-combined_bow                  0.646031          0.538136   \n",
       "SVC-combined_w2v_mean             0.571619          0.713983   \n",
       "\n",
       "                      train_precision_score train_recall_score  \n",
       "SVC-combined_bow                   0.959229           0.993214  \n",
       "SVC-combined_w2v_mean              0.714626           0.888027  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df.index.isin([best_recall, best_precision])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Support Vector Machines result in the best accuracy - both in terms of precision and recall. Combining the plots from both data sources also results in the best accuracy, which intuitively makes sense as combining the sources results in a more descriptive plot.\n",
    "\n",
    "Bag-of-words results in the best `test_precision_score`, although the model appears to be overfitting. Perhaps increasing the penalty parameter of the error term would result in better test scores, although our existing results are sufficient enough to move forward without exploring that option.\n",
    "\n",
    "Words2Vectors results in the best `test_recall_score`, although the test_precision_score is less accurate than the results from the bag-of-words model.\n",
    "\n",
    "Next we will look at the full results of each model by using the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  SVC-combined_w2v_mean\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.57      0.74      0.65       137\n",
      "        Fantasy       0.42      0.67      0.52        82\n",
      "      Animation       0.82      1.00      0.90        78\n",
      "          Drama       0.82      0.88      0.85       515\n",
      "         Horror       0.91      1.00      0.95        41\n",
      "         Action       0.57      0.90      0.70       134\n",
      "         Comedy       0.58      0.86      0.69       189\n",
      "        History       0.72      1.00      0.84        55\n",
      "        Western       0.79      1.00      0.88        23\n",
      "       Thriller       0.55      0.79      0.65       188\n",
      "          Crime       0.64      0.82      0.72       148\n",
      "    Documentary       0.92      1.00      0.96        83\n",
      "Science Fiction       0.77      1.00      0.87        87\n",
      "        Mystery       0.88      1.00      0.94        37\n",
      "          Music       0.69      0.99      0.82       119\n",
      "        Romance       0.90      1.00      0.95        91\n",
      "         Family       0.96      1.00      0.98        51\n",
      "            War       1.00      1.00      1.00         5\n",
      "\n",
      "    avg / total       0.71      0.89      0.79      2063\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.49      0.73      0.59        30\n",
      "        Fantasy       0.14      0.36      0.21        11\n",
      "      Animation       0.70      0.70      0.70        20\n",
      "          Drama       0.85      0.91      0.88       128\n",
      "         Horror       0.42      0.45      0.43        11\n",
      "         Action       0.54      0.88      0.67        32\n",
      "         Comedy       0.51      0.81      0.63        47\n",
      "        History       0.38      0.47      0.42        17\n",
      "        Western       0.89      0.80      0.84        10\n",
      "       Thriller       0.39      0.61      0.47        36\n",
      "          Crime       0.49      0.71      0.58        28\n",
      "    Documentary       0.50      0.39      0.44        23\n",
      "Science Fiction       0.18      0.31      0.23        13\n",
      "        Mystery       0.10      0.14      0.12         7\n",
      "          Music       0.38      0.62      0.47        29\n",
      "        Romance       0.54      0.65      0.59        20\n",
      "         Family       0.58      0.78      0.67         9\n",
      "            War       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.57      0.71      0.63       472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results: ', best_recall)\n",
    "print(resultsDict[best_recall]['train_classification_report'])\n",
    "print(resultsDict[best_recall]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  SVC-combined_bow\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.94      1.00      0.97       137\n",
      "        Fantasy       0.95      1.00      0.98        82\n",
      "      Animation       0.95      1.00      0.97        78\n",
      "          Drama       1.00      0.98      0.99       515\n",
      "         Horror       0.98      1.00      0.99        41\n",
      "         Action       0.94      1.00      0.97       134\n",
      "         Comedy       0.94      1.00      0.97       189\n",
      "        History       0.96      1.00      0.98        55\n",
      "        Western       0.96      1.00      0.98        23\n",
      "       Thriller       0.94      1.00      0.97       188\n",
      "          Crime       0.95      1.00      0.98       148\n",
      "    Documentary       0.98      1.00      0.99        83\n",
      "Science Fiction       0.98      1.00      0.99        87\n",
      "        Mystery       1.00      1.00      1.00        37\n",
      "          Music       0.94      1.00      0.97       119\n",
      "        Romance       0.98      1.00      0.99        91\n",
      "         Family       0.86      0.96      0.91        51\n",
      "            War       0.01      1.00      0.01         5\n",
      "\n",
      "    avg / total       0.96      0.99      0.97      2063\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.59      0.53      0.56        30\n",
      "        Fantasy       0.14      0.09      0.11        11\n",
      "      Animation       0.62      0.25      0.36        20\n",
      "          Drama       0.82      0.84      0.83       128\n",
      "         Horror       0.67      0.18      0.29        11\n",
      "         Action       0.67      0.56      0.61        32\n",
      "         Comedy       0.55      0.47      0.51        47\n",
      "        History       0.71      0.29      0.42        17\n",
      "        Western       1.00      0.30      0.46        10\n",
      "       Thriller       0.45      0.42      0.43        36\n",
      "          Crime       0.48      0.54      0.51        28\n",
      "    Documentary       0.87      0.57      0.68        23\n",
      "Science Fiction       0.71      0.38      0.50        13\n",
      "        Mystery       0.20      0.14      0.17         7\n",
      "          Music       0.48      0.34      0.40        29\n",
      "        Romance       0.50      0.30      0.37        20\n",
      "         Family       0.73      0.89      0.80         9\n",
      "            War       0.01      1.00      0.01         1\n",
      "\n",
      "    avg / total       0.65      0.54      0.57       472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results: ', best_precision)\n",
    "print(resultsDict[best_precision]['train_classification_report'])\n",
    "print(resultsDict[best_precision]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: add analysis for the classification reports, listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_bow</th>\n",
       "      <td>0.637348</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.929714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_w2v_mean</th>\n",
       "      <td>0.449889</td>\n",
       "      <td>0.290254</td>\n",
       "      <td>0.545585</td>\n",
       "      <td>0.270965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_bow</th>\n",
       "      <td>0.552091</td>\n",
       "      <td>0.345339</td>\n",
       "      <td>0.981623</td>\n",
       "      <td>0.81047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_w2v_mean</th>\n",
       "      <td>0.240398</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.490658</td>\n",
       "      <td>0.254484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_bow</th>\n",
       "      <td>0.588559</td>\n",
       "      <td>0.324153</td>\n",
       "      <td>0.941385</td>\n",
       "      <td>0.689772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_w2v_mean</th>\n",
       "      <td>0.253401</td>\n",
       "      <td>0.275424</td>\n",
       "      <td>0.45296</td>\n",
       "      <td>0.261755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_bow</th>\n",
       "      <td>0.173559</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.160703</td>\n",
       "      <td>0.249636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_w2v_mean</th>\n",
       "      <td>0.613718</td>\n",
       "      <td>0.434322</td>\n",
       "      <td>0.682537</td>\n",
       "      <td>0.457586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_bow</th>\n",
       "      <td>0.173559</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.160703</td>\n",
       "      <td>0.249636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_w2v_mean</th>\n",
       "      <td>0.404421</td>\n",
       "      <td>0.360169</td>\n",
       "      <td>0.573536</td>\n",
       "      <td>0.400873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_bow</th>\n",
       "      <td>0.173559</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.160703</td>\n",
       "      <td>0.249636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_w2v_mean</th>\n",
       "      <td>0.407053</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.506525</td>\n",
       "      <td>0.416869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_bow</th>\n",
       "      <td>0.646031</td>\n",
       "      <td>0.538136</td>\n",
       "      <td>0.959229</td>\n",
       "      <td>0.993214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_w2v_mean</th>\n",
       "      <td>0.571619</td>\n",
       "      <td>0.713983</td>\n",
       "      <td>0.714626</td>\n",
       "      <td>0.888027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-imdb_bow</th>\n",
       "      <td>0.606078</td>\n",
       "      <td>0.527542</td>\n",
       "      <td>0.935169</td>\n",
       "      <td>0.991275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-imdb_w2v_mean</th>\n",
       "      <td>0.553211</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.700093</td>\n",
       "      <td>0.863306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_bow</th>\n",
       "      <td>0.57179</td>\n",
       "      <td>0.548729</td>\n",
       "      <td>0.828317</td>\n",
       "      <td>0.960252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_w2v_mean</th>\n",
       "      <td>0.546045</td>\n",
       "      <td>0.646186</td>\n",
       "      <td>0.72682</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              test_precision_score test_recall_score  \\\n",
       "Naive-Bayes-combined_bow                  0.637348          0.381356   \n",
       "Naive-Bayes-combined_w2v_mean             0.449889          0.290254   \n",
       "Naive-Bayes-imdb_bow                      0.552091          0.345339   \n",
       "Naive-Bayes-imdb_w2v_mean                 0.240398          0.273305   \n",
       "Naive-Bayes-tmdb_bow                      0.588559          0.324153   \n",
       "Naive-Bayes-tmdb_w2v_mean                 0.253401          0.275424   \n",
       "SGD-combined_bow                          0.173559          0.271186   \n",
       "SGD-combined_w2v_mean                     0.613718          0.434322   \n",
       "SGD-imdb_bow                              0.173559          0.271186   \n",
       "SGD-imdb_w2v_mean                         0.404421          0.360169   \n",
       "SGD-tmdb_bow                              0.173559          0.271186   \n",
       "SGD-tmdb_w2v_mean                         0.407053           0.40678   \n",
       "SVC-combined_bow                          0.646031          0.538136   \n",
       "SVC-combined_w2v_mean                     0.571619          0.713983   \n",
       "SVC-imdb_bow                              0.606078          0.527542   \n",
       "SVC-imdb_w2v_mean                         0.553211          0.677966   \n",
       "SVC-tmdb_bow                               0.57179          0.548729   \n",
       "SVC-tmdb_w2v_mean                         0.546045          0.646186   \n",
       "\n",
       "                              train_precision_score train_recall_score  \n",
       "Naive-Bayes-combined_bow                   0.981919           0.929714  \n",
       "Naive-Bayes-combined_w2v_mean              0.545585           0.270965  \n",
       "Naive-Bayes-imdb_bow                       0.981623            0.81047  \n",
       "Naive-Bayes-imdb_w2v_mean                  0.490658           0.254484  \n",
       "Naive-Bayes-tmdb_bow                       0.941385           0.689772  \n",
       "Naive-Bayes-tmdb_w2v_mean                   0.45296           0.261755  \n",
       "SGD-combined_bow                           0.160703           0.249636  \n",
       "SGD-combined_w2v_mean                      0.682537           0.457586  \n",
       "SGD-imdb_bow                               0.160703           0.249636  \n",
       "SGD-imdb_w2v_mean                          0.573536           0.400873  \n",
       "SGD-tmdb_bow                               0.160703           0.249636  \n",
       "SGD-tmdb_w2v_mean                          0.506525           0.416869  \n",
       "SVC-combined_bow                           0.959229           0.993214  \n",
       "SVC-combined_w2v_mean                      0.714626           0.888027  \n",
       "SVC-imdb_bow                               0.935169           0.991275  \n",
       "SVC-imdb_w2v_mean                          0.700093           0.863306  \n",
       "SVC-tmdb_bow                               0.828317           0.960252  \n",
       "SVC-tmdb_w2v_mean                           0.72682              0.873  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
