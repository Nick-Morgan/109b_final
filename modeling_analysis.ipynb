{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Classification Team 11\n",
    "\n",
    "# Modeling and Analysis\n",
    "\n",
    "### Team Members:\n",
    "Andrew Lund, Nicholas Morgan, Amay Umradia, Charles Webb\n",
    "\n",
    "The main purpose of this notebook is to systematically evaluate many models and store their results in an easy-to-manipulate format. We begin by creating a function to create and store our model results, and then \n",
    "\n",
    "**This notebook accomplishes two primary tasks:**\n",
    "1. Systematically evaluate many models and store their results in an easy-to-manipulate format. We achieve this by looping through multiple dictionaries to evaluate on a common function.\n",
    "2. Analyze the results of the models to identify patterns between data sources, models, and predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be trying a number of different models, we decided to create a function that will store all of the outcomes of interest in a dictionary. The function below does an 80/20 train/test split. We also fit all of our models using a 50/50 split, but had better accuracy when using 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, predictors, response, cv=False, params=None):\n",
    "    \"\"\"\n",
    "    evaluate_model()\n",
    "    \n",
    "    -splits the predictors & response variables into train and test sets. \n",
    "    -creates a dictionary of model outcomes that are of interest\n",
    "    -if specified, this function will use cross-validation to determine the optimal parameters for a given model\n",
    "    \n",
    "    inputs:\n",
    "        -model: a model object to be fitted\n",
    "        -predictors: an array, series, or dataframe of predictor variable(s)\n",
    "        -response: an array or series of the response variable\n",
    "        -cv: whether or not to cross-validate the model's parameters (default=False)\n",
    "        -params: if cv=True, params are required to indicate what parameters to optimize in the given model (default=None)\n",
    "        \n",
    "    outputs:\n",
    "        -a results dictionary containing the following:\n",
    "            -a fitted model object\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    train_x, test_x = train_test_split(predictors, test_size=0.2, random_state=9001)\n",
    "    train_y, test_y = train_test_split(response, test_size=0.2, random_state=9001)\n",
    "    \n",
    "    if cv:\n",
    "        model = GridSearchCV(model, params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    \n",
    "    classif = OneVsRestClassifier(model)\n",
    "    classif.fit(train_x, train_y)\n",
    "    \n",
    "    train_yhat = classif.predict(train_x)\n",
    "    test_yhat = classif.predict(test_x)\n",
    "    \n",
    "    results['fitted_model'] = classif\n",
    "    \n",
    "    results['train_yhat'] = train_yhat\n",
    "    results['test_yhat'] = test_yhat\n",
    "    \n",
    "    results['train_recall_score'] = recall_score(train_y, train_yhat, average='weighted')\n",
    "    results['test_recall_score'] = recall_score(test_y, test_yhat, average='weighted')\n",
    "    \n",
    "    results['train_precision_score'] = precision_score(train_y, train_yhat,average='weighted')\n",
    "    results['test_precision_score'] = precision_score(test_y, test_yhat,average='weighted')\n",
    "    \n",
    "    results['train_classification_report'] = classification_report(train_y, train_yhat,target_names=target_names)\n",
    "    results['test_classification_report'] = classification_report(test_y, test_yhat,target_names=target_names)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we created the [MultiLabel Binarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer) array in a previous notebook, it created 18 classes corresponding to the 18 genres found in our dataset. These classes were labeled as class 0 through class 17, sorted in numeric order of the genre id. In order to improve readability of our report, we will set the target_names to be the name of the genre, rather than the id. The target names need to be in the same order as the MultiLabel Binarizer, so we will do two steps:\n",
    "\n",
    "    1: Sort the keys of the id_to_genre dictionary in ascending numeric order\n",
    "    2: Use the sorted keys against the id_to_genre dictionary to create the ordered target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "id_to_genre = json.load(open('data/id_to_genre.json'))\n",
    "\n",
    "id_to_genre = {int(key):value for key, value in id_to_genre.items()} #convert string keys to int keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adventure',\n",
       " 'Fantasy',\n",
       " 'Animation',\n",
       " 'Drama',\n",
       " 'Horror',\n",
       " 'Action',\n",
       " 'Comedy',\n",
       " 'History',\n",
       " 'Western',\n",
       " 'Thriller',\n",
       " 'Crime',\n",
       " 'Science Fiction',\n",
       " 'Mystery',\n",
       " 'Music',\n",
       " 'Romance',\n",
       " 'Family',\n",
       " 'War',\n",
       " 'TV Movie']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = json.load(open('data/target_names.json'))['tmdb']\n",
    "\n",
    "target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load in our arrays to be used as predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmdb_bow = np.load('data/tmdb_bow.npy')\n",
    "imdb_bow = np.load('data/imdb_bow.npy')\n",
    "combined_bow = np.load('data/combined_bow.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Word2Vector and Docs2Vector arrays require 2 additional steps in order to be used as predictors:\n",
    "\n",
    "    1: They need to be converted to an array of lists (they are currently an array of arrays, which is incompatible with the structure of the response variable, which is also an array of lists).\n",
    "    2: The values need to be standardized between 0 and 1, because (todo - there was an error when they were negative. I will have to re-run and see what caused the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmdb_w2v_mean = np.load('data/tmdb_w2v_mean.npy')\n",
    "imdb_w2v_mean = np.load('data/imdb_w2v_mean.npy')\n",
    "combined_w2v_mean = np.load('data/combined_w2v_mean.npy')\n",
    "\n",
    "tmdb_doc_vec = np.load('data/tmdb_doc_vec.npy')\n",
    "imdb_doc_vec = np.load('data/imdb_doc_vec.npy')\n",
    "combined_doc_vec = np.load('data/combined_doc_vec.npy')\n",
    "\n",
    "tmdb_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, tmdb_w2v_mean)\n",
    "imdb_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, imdb_w2v_mean)\n",
    "combined_w2v_mean = np.apply_along_axis(lambda x: list(x), 0, combined_w2v_mean)\n",
    "\n",
    "tmdb_doc_vec = np.apply_along_axis(lambda x: list(x), 0, tmdb_doc_vec)\n",
    "imdb_doc_vec = np.apply_along_axis(lambda x: list(x), 0, imdb_doc_vec)\n",
    "combined_doc_vec = np.apply_along_axis(lambda x: list(x), 0, combined_doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "\n",
    "#word2vec scaling\n",
    "scale.fit(tmdb_w2v_mean)\n",
    "tmdb_w2v_mean = scale.transform(tmdb_w2v_mean)\n",
    "\n",
    "scale.fit(imdb_w2v_mean)\n",
    "imdb_w2v_mean = scale.transform(imdb_w2v_mean)\n",
    "\n",
    "scale.fit(combined_w2v_mean)\n",
    "combined_w2v_mean = scale.transform(combined_w2v_mean)\n",
    "\n",
    "#doc2vec scaling\n",
    "scale.fit(tmdb_doc_vec)\n",
    "tmdb_doc_vec = scale.transform(tmdb_doc_vec)\n",
    "\n",
    "scale.fit(imdb_doc_vec)\n",
    "imdb_doc_vec = scale.transform(imdb_doc_vec)\n",
    "\n",
    "scale.fit(combined_doc_vec)\n",
    "combined_doc_vec = scale.transform(combined_doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_tmdb = np.load('data/binary_tmdb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all of the parameters necessary to run our function. We will create models using 6 different predictors:\n",
    "\n",
    "    1: Bag of words using the TMDB plot\n",
    "    2: Bag of words using the IMDB plot\n",
    "    3: Bag of words using the combined plots from both sources\n",
    "    4: Word2Vectors using the TMDB plot\n",
    "    5: Word2Vectors using the IMDB plot\n",
    "    6: Word2Vectors using the combined plots from both sources\n",
    "    7: Doc2Vectors using the TMDB plot\n",
    "    8: Doc2Vectors using the IMDB plot\n",
    "    9: Doc2Vectors using the combined plots from both sources\n",
    "    \n",
    "\n",
    "We will use these predictors to create 3 classification models to predict movie genres:\n",
    "\n",
    "    1: Naive-Bayes, with a cross-validated smoothing parameter\n",
    "    2: Stochastic Gradient Descent, with a cross-validated regularization multiplier.\n",
    "    3: Support Vector machines, with a cross-validated penalty parameter of the error term.\n",
    "    \n",
    "    \n",
    "This will result in 27 total models being created. We will store the results of each model in a dictionary, which will allow us to identify the best-performing models.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelDict = {'Naive-Bayes':{'model':MultinomialNB(),\n",
    "                           'params':{'alpha':[0.01,0.1,1.0]}},\n",
    "            \n",
    "            'SGD':{'model':SGDClassifier(loss='hinge',penalty='l2',n_iter=5,random_state=9001),\n",
    "                   'params':{'alpha':[0.01,0.1,1.0]}},\n",
    "            \n",
    "            'SVC':{'model':SVC(class_weight='balanced', kernel='linear'),\n",
    "                   'params':{'C':[0.01,0.1,1.0]}}\n",
    "           }\n",
    "\n",
    "predictorDict = {\n",
    "                 'tmdb_bow':tmdb_bow,\n",
    "                 'imdb_bow':imdb_bow,\n",
    "                 'combined_bow':combined_bow,\n",
    "                 'tmdb_w2v_mean':tmdb_w2v_mean,\n",
    "                 'imdb_w2v_mean':imdb_w2v_mean,\n",
    "                 'combined_w2v_mean':combined_w2v_mean,\n",
    "                 'tmdb_doc_vec':tmdb_doc_vec,\n",
    "                 'imdb_doc_vec':imdb_doc_vec,\n",
    "                 'combined_doc_vec':combined_doc_vec\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn returns a warning when the function above uses weighted averages on samples that have no predictors. This will not affect our metrics and outputs repetitive information when run in a loop. The warning, that we are choosing to ignore, is as follows:\n",
    "\n",
    "```UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
    "  'precision', 'predicted', average, warn_for)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resultsDict = {}\n",
    "import warnings\n",
    "with warnings.catch_warnings(): #temporarily ignore the warnings described above\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for model in modelDict:\n",
    "        for predictor in predictorDict:\n",
    "            resultsDict['{0}-{1}'.format(model,predictor)] = evaluate_model(model = modelDict[model]['model'],\n",
    "                                                                            predictors = predictorDict[predictor], \n",
    "                                                                            response = binary_tmdb,\n",
    "                                                                            cv=True,\n",
    "                                                                            params=modelDict[model]['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This next cell will be removed when we submit our final report. It is just being used now to temporarily store the results of the models to avoid having to re-run the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle #trying to use pickle to see if the model object can be retained while saving, as opposed to json which drops it\n",
    "\n",
    "pickle.dump(resultsDict, open('data/resultsDict.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start_here\n",
    "\n",
    "import pickle\n",
    "\n",
    "resultsDict = pickle.load(open('data/resultsDict.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will make a dataframe of the scores for the sake of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = ['train_recall_score','test_recall_score',\n",
    "          'train_precision_score','test_precision_score']\n",
    "\n",
    "results_df = pd.DataFrame(resultsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics\n",
    "\n",
    "\n",
    "**Precision Score, Recall Score:**\n",
    "\n",
    "As shown in the cell above, we are interested in the precision score as well as the recall score. \n",
    "\n",
    "\n",
    "The precision score can be defined as `tp / (tp + fp)`, where `tp` represents true-positives and `fp` represents false-positives.\n",
    "\n",
    "The recall score can be defined as `tp / (tp + fn)` where `tp` represents true-positives and `fn` represents false-negatives\n",
    "\n",
    "\n",
    "In other words, the precision score penalizes false-positives, whereas the recall score penalizes false-negatives. Both scores range from 0 to 1, with 1 representing perfect accuracy.\n",
    "\n",
    "\n",
    "**F1-Score:**\n",
    "\n",
    "While not included in our results_df dataframe, this outcome is included in the `classification_report`, which is shown a little further below. The F1 score can be defined as:\n",
    "\n",
    "`F1 = 2 * (precision * recall) / (precision + recall)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = results_df.loc[results_df.index.isin(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_bow</th>\n",
       "      <td>0.725255</td>\n",
       "      <td>0.377953</td>\n",
       "      <td>0.982521</td>\n",
       "      <td>0.926914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_w2v_mean</th>\n",
       "      <td>0.456782</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.583491</td>\n",
       "      <td>0.274074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_bow</th>\n",
       "      <td>0.596821</td>\n",
       "      <td>0.340551</td>\n",
       "      <td>0.948176</td>\n",
       "      <td>0.780741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_w2v_mean</th>\n",
       "      <td>0.178442</td>\n",
       "      <td>0.253937</td>\n",
       "      <td>0.605336</td>\n",
       "      <td>0.262716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_bow</th>\n",
       "      <td>0.619958</td>\n",
       "      <td>0.301181</td>\n",
       "      <td>0.970122</td>\n",
       "      <td>0.79358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_w2v_mean</th>\n",
       "      <td>0.316826</td>\n",
       "      <td>0.261811</td>\n",
       "      <td>0.494266</td>\n",
       "      <td>0.265185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_w2v_mean</th>\n",
       "      <td>0.514741</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.602544</td>\n",
       "      <td>0.42963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_w2v_mean</th>\n",
       "      <td>0.380598</td>\n",
       "      <td>0.444882</td>\n",
       "      <td>0.539381</td>\n",
       "      <td>0.497284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_w2v_mean</th>\n",
       "      <td>0.470563</td>\n",
       "      <td>0.379921</td>\n",
       "      <td>0.619262</td>\n",
       "      <td>0.430617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_bow</th>\n",
       "      <td>0.678057</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.95214</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_doc_vec</th>\n",
       "      <td>0.186744</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.532346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_w2v_mean</th>\n",
       "      <td>0.558305</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.742788</td>\n",
       "      <td>0.893827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-imdb_bow</th>\n",
       "      <td>0.607838</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.87196</td>\n",
       "      <td>0.974321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-imdb_doc_vec</th>\n",
       "      <td>0.153214</td>\n",
       "      <td>0.340551</td>\n",
       "      <td>0.162445</td>\n",
       "      <td>0.37679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-imdb_w2v_mean</th>\n",
       "      <td>0.537334</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.681017</td>\n",
       "      <td>0.864691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_bow</th>\n",
       "      <td>0.475574</td>\n",
       "      <td>0.496063</td>\n",
       "      <td>0.815313</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_doc_vec</th>\n",
       "      <td>0.186744</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.532346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_w2v_mean</th>\n",
       "      <td>0.540201</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.748116</td>\n",
       "      <td>0.877531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              test_precision_score test_recall_score  \\\n",
       "Naive-Bayes-combined_bow                  0.725255          0.377953   \n",
       "Naive-Bayes-combined_doc_vec              0.166339          0.255906   \n",
       "Naive-Bayes-combined_w2v_mean             0.456782          0.267717   \n",
       "Naive-Bayes-imdb_bow                      0.596821          0.340551   \n",
       "Naive-Bayes-imdb_doc_vec                  0.166339          0.255906   \n",
       "Naive-Bayes-imdb_w2v_mean                 0.178442          0.253937   \n",
       "Naive-Bayes-tmdb_bow                      0.619958          0.301181   \n",
       "Naive-Bayes-tmdb_doc_vec                  0.166339          0.255906   \n",
       "Naive-Bayes-tmdb_w2v_mean                 0.316826          0.261811   \n",
       "SGD-combined_bow                          0.166339          0.255906   \n",
       "SGD-combined_doc_vec                      0.166339          0.255906   \n",
       "SGD-combined_w2v_mean                     0.514741          0.409449   \n",
       "SGD-imdb_bow                              0.166339          0.255906   \n",
       "SGD-imdb_doc_vec                          0.166339          0.255906   \n",
       "SGD-imdb_w2v_mean                         0.380598          0.444882   \n",
       "SGD-tmdb_bow                              0.166339          0.255906   \n",
       "SGD-tmdb_doc_vec                          0.166339          0.255906   \n",
       "SGD-tmdb_w2v_mean                         0.470563          0.379921   \n",
       "SVC-combined_bow                          0.678057          0.525591   \n",
       "SVC-combined_doc_vec                      0.186744          0.525591   \n",
       "SVC-combined_w2v_mean                     0.558305          0.688976   \n",
       "SVC-imdb_bow                              0.607838          0.551181   \n",
       "SVC-imdb_doc_vec                          0.153214          0.340551   \n",
       "SVC-imdb_w2v_mean                         0.537334          0.685039   \n",
       "SVC-tmdb_bow                              0.475574          0.496063   \n",
       "SVC-tmdb_doc_vec                          0.186744          0.525591   \n",
       "SVC-tmdb_w2v_mean                         0.540201          0.633858   \n",
       "\n",
       "                              train_precision_score train_recall_score  \n",
       "Naive-Bayes-combined_bow                   0.982521           0.926914  \n",
       "Naive-Bayes-combined_doc_vec               0.163924           0.254321  \n",
       "Naive-Bayes-combined_w2v_mean              0.583491           0.274074  \n",
       "Naive-Bayes-imdb_bow                       0.948176           0.780741  \n",
       "Naive-Bayes-imdb_doc_vec                   0.163924           0.254321  \n",
       "Naive-Bayes-imdb_w2v_mean                  0.605336           0.262716  \n",
       "Naive-Bayes-tmdb_bow                       0.970122            0.79358  \n",
       "Naive-Bayes-tmdb_doc_vec                   0.163924           0.254321  \n",
       "Naive-Bayes-tmdb_w2v_mean                  0.494266           0.265185  \n",
       "SGD-combined_bow                           0.163924           0.254321  \n",
       "SGD-combined_doc_vec                       0.163924           0.254321  \n",
       "SGD-combined_w2v_mean                      0.602544            0.42963  \n",
       "SGD-imdb_bow                               0.163924           0.254321  \n",
       "SGD-imdb_doc_vec                           0.163924           0.254321  \n",
       "SGD-imdb_w2v_mean                          0.539381           0.497284  \n",
       "SGD-tmdb_bow                               0.163924           0.254321  \n",
       "SGD-tmdb_doc_vec                           0.163924           0.254321  \n",
       "SGD-tmdb_w2v_mean                          0.619262           0.430617  \n",
       "SVC-combined_bow                            0.95214           0.985679  \n",
       "SVC-combined_doc_vec                       0.194323           0.532346  \n",
       "SVC-combined_w2v_mean                      0.742788           0.893827  \n",
       "SVC-imdb_bow                                0.87196           0.974321  \n",
       "SVC-imdb_doc_vec                           0.162445            0.37679  \n",
       "SVC-imdb_w2v_mean                          0.681017           0.864691  \n",
       "SVC-tmdb_bow                               0.815313           0.946667  \n",
       "SVC-tmdb_doc_vec                           0.194323           0.532346  \n",
       "SVC-tmdb_w2v_mean                          0.748116           0.877531  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.transpose()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that many of the models listed above have identical scores for all outcomes. Let's extract those models specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-imdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-tmdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-combined_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-imdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD-tmdb_doc_vec</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.254321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_doc_vec</th>\n",
       "      <td>0.186744</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.532346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-tmdb_doc_vec</th>\n",
       "      <td>0.186744</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.532346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             test_precision_score test_recall_score  \\\n",
       "Naive-Bayes-combined_doc_vec             0.166339          0.255906   \n",
       "Naive-Bayes-imdb_doc_vec                 0.166339          0.255906   \n",
       "Naive-Bayes-tmdb_doc_vec                 0.166339          0.255906   \n",
       "SGD-combined_bow                         0.166339          0.255906   \n",
       "SGD-combined_doc_vec                     0.166339          0.255906   \n",
       "SGD-imdb_bow                             0.166339          0.255906   \n",
       "SGD-imdb_doc_vec                         0.166339          0.255906   \n",
       "SGD-tmdb_bow                             0.166339          0.255906   \n",
       "SGD-tmdb_doc_vec                         0.166339          0.255906   \n",
       "SVC-combined_doc_vec                     0.186744          0.525591   \n",
       "SVC-tmdb_doc_vec                         0.186744          0.525591   \n",
       "\n",
       "                             train_precision_score train_recall_score  \n",
       "Naive-Bayes-combined_doc_vec              0.163924           0.254321  \n",
       "Naive-Bayes-imdb_doc_vec                  0.163924           0.254321  \n",
       "Naive-Bayes-tmdb_doc_vec                  0.163924           0.254321  \n",
       "SGD-combined_bow                          0.163924           0.254321  \n",
       "SGD-combined_doc_vec                      0.163924           0.254321  \n",
       "SGD-imdb_bow                              0.163924           0.254321  \n",
       "SGD-imdb_doc_vec                          0.163924           0.254321  \n",
       "SGD-tmdb_bow                              0.163924           0.254321  \n",
       "SGD-tmdb_doc_vec                          0.163924           0.254321  \n",
       "SVC-combined_doc_vec                      0.194323           0.532346  \n",
       "SVC-tmdb_doc_vec                          0.194323           0.532346  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_scores = pd.concat(group for _, group in results_df.groupby((scores)) if len(group) > 1)\n",
    "duplicate_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 duplicate values occuring. One occurs in 9 models, and the other occurs with 2. We will explain these one at a time, beginning with the group of 9 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1 = list(duplicate_scores.index.values[:9])\n",
    "group_2 = list(duplicate_scores.index.values[9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report shows the issue occuring in group 1. Instead of printing out 2 reports for each of the 9 models, we will instead write code that proves that all 9 models have identical classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in range(len(group_1)):\n",
    "    if resultsDict[group_1[0]]['train_classification_report'] != resultsDict[group_1[model]]['train_classification_report']:\n",
    "        print('Train classification reports do not match between model 0 and model {}'.format(model))\n",
    "    \n",
    "    if resultsDict[group_1[0]]['test_classification_report'] != resultsDict[group_1[model]]['test_classification_report']:\n",
    "        print('Test classification reports do not match between model 0 and model {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have proven that all 9 models have identical classification reports, we can print a single set of reports to observe what is happening across all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.00      0.00      0.00       137\n",
      "        Fantasy       0.00      0.00      0.00        76\n",
      "      Animation       0.00      0.00      0.00        82\n",
      "          Drama       0.64      1.00      0.78       515\n",
      "         Horror       0.00      0.00      0.00        41\n",
      "         Action       0.00      0.00      0.00       124\n",
      "         Comedy       0.00      0.00      0.00       189\n",
      "        History       0.00      0.00      0.00        54\n",
      "        Western       0.00      0.00      0.00        25\n",
      "       Thriller       0.00      0.00      0.00       184\n",
      "          Crime       0.00      0.00      0.00       143\n",
      "Science Fiction       0.00      0.00      0.00        81\n",
      "        Mystery       0.00      0.00      0.00        77\n",
      "          Music       0.00      0.00      0.00        34\n",
      "        Romance       0.00      0.00      0.00       124\n",
      "         Family       0.00      0.00      0.00        92\n",
      "            War       0.00      0.00      0.00        42\n",
      "       TV Movie       0.00      0.00      0.00         5\n",
      "\n",
      "    avg / total       0.16      0.25      0.20      2025\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.00      0.00      0.00        31\n",
      "        Fantasy       0.00      0.00      0.00        17\n",
      "      Animation       0.00      0.00      0.00        17\n",
      "          Drama       0.65      1.00      0.79       130\n",
      "         Horror       0.00      0.00      0.00        11\n",
      "         Action       0.00      0.00      0.00        40\n",
      "         Comedy       0.00      0.00      0.00        49\n",
      "        History       0.00      0.00      0.00        19\n",
      "        Western       0.00      0.00      0.00         8\n",
      "       Thriller       0.00      0.00      0.00        39\n",
      "          Crime       0.00      0.00      0.00        33\n",
      "Science Fiction       0.00      0.00      0.00        25\n",
      "        Mystery       0.00      0.00      0.00        19\n",
      "          Music       0.00      0.00      0.00         5\n",
      "        Romance       0.00      0.00      0.00        28\n",
      "         Family       0.00      0.00      0.00        17\n",
      "            War       0.00      0.00      0.00        19\n",
      "       TV Movie       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.17      0.26      0.20       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resultsDict[group_1[0]]['train_classification_report'])\n",
    "print(resultsDict[group_1[0]]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 9 models are being overfit - for both the train and test dataset, each of these models is predicting that 100% of the movies will be genres. This is because of the large number of drama movies in the dataset (this is demonstrated in our EDA notebook). Fine-tuning the penalization parameters may help with these models being overfit, but that exploration is outside the scope of this analysis. We will instead focus on the models that are not being overfit, after first exploring the other group of duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's make sure the classification reports are identical between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if resultsDict[group_2[0]]['train_classification_report'] != resultsDict[group_2[1]]['train_classification_report']:\n",
    "    print('Train classification reports do not match between model 0 and model 1')\n",
    "    \n",
    "if resultsDict[group_2[0]]['test_classification_report'] != resultsDict[group_2[1]]['test_classification_report']:\n",
    "    print('Test classification reports do not match between model 0 and model 1')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two reports are identical, so let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.29      0.72      0.41       137\n",
      "        Fantasy       0.10      1.00      0.17        76\n",
      "      Animation       0.10      1.00      0.19        82\n",
      "          Drama       0.00      0.00      0.00       515\n",
      "         Horror       0.05      1.00      0.10        41\n",
      "         Action       0.32      0.73      0.44       124\n",
      "         Comedy       0.53      0.16      0.24       189\n",
      "        History       0.14      0.69      0.23        54\n",
      "        Western       0.11      0.76      0.19        25\n",
      "       Thriller       0.23      1.00      0.37       184\n",
      "          Crime       0.18      1.00      0.30       143\n",
      "Science Fiction       0.28      0.64      0.39        81\n",
      "        Mystery       0.16      0.77      0.27        77\n",
      "          Music       0.09      0.76      0.16        34\n",
      "        Romance       0.45      0.36      0.40       124\n",
      "         Family       0.24      0.59      0.34        92\n",
      "            War       0.07      0.98      0.13        42\n",
      "       TV Movie       0.00      0.00      0.00         5\n",
      "\n",
      "    avg / total       0.19      0.53      0.23      2025\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.26      0.74      0.38        31\n",
      "        Fantasy       0.09      1.00      0.16        17\n",
      "      Animation       0.09      1.00      0.16        17\n",
      "          Drama       0.00      0.00      0.00       130\n",
      "         Horror       0.06      1.00      0.10        11\n",
      "         Action       0.36      0.68      0.47        40\n",
      "         Comedy       0.50      0.14      0.22        49\n",
      "        History       0.17      0.58      0.27        19\n",
      "        Western       0.14      0.75      0.24         8\n",
      "       Thriller       0.20      1.00      0.33        39\n",
      "          Crime       0.17      1.00      0.28        33\n",
      "Science Fiction       0.32      0.72      0.44        25\n",
      "        Mystery       0.15      0.68      0.24        19\n",
      "          Music       0.04      0.60      0.08         5\n",
      "        Romance       0.38      0.39      0.39        28\n",
      "         Family       0.16      0.71      0.26        17\n",
      "            War       0.13      1.00      0.23        19\n",
      "       TV Movie       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.19      0.53      0.22       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resultsDict[group_2[0]]['train_classification_report'])\n",
    "\n",
    "print(resultsDict[group_2[0]]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVC-combined_doc_vec', 'SVC-tmdb_doc_vec']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_recall = results_df['test_recall_score'].idxmax()\n",
    "best_precision = results_df['test_precision_score'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_precision_score</th>\n",
       "      <th>test_recall_score</th>\n",
       "      <th>train_precision_score</th>\n",
       "      <th>train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive-Bayes-combined_bow</th>\n",
       "      <td>0.725255</td>\n",
       "      <td>0.377953</td>\n",
       "      <td>0.982521</td>\n",
       "      <td>0.926914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC-combined_w2v_mean</th>\n",
       "      <td>0.558305</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.742788</td>\n",
       "      <td>0.893827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         test_precision_score test_recall_score  \\\n",
       "Naive-Bayes-combined_bow             0.725255          0.377953   \n",
       "SVC-combined_w2v_mean                0.558305          0.688976   \n",
       "\n",
       "                         train_precision_score train_recall_score  \n",
       "Naive-Bayes-combined_bow              0.982521           0.926914  \n",
       "SVC-combined_w2v_mean                 0.742788           0.893827  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df.index.isin([best_recall, best_precision])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the top models based on precision as well as recall scores, we can take a closer look through the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  Naive-Bayes-combined_bow\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.97      0.88      0.92       137\n",
      "        Fantasy       1.00      0.72      0.84        76\n",
      "      Animation       1.00      1.00      1.00        82\n",
      "          Drama       0.96      0.99      0.98       515\n",
      "         Horror       1.00      1.00      1.00        41\n",
      "         Action       0.98      0.88      0.93       124\n",
      "         Comedy       1.00      0.94      0.97       189\n",
      "        History       1.00      1.00      1.00        54\n",
      "        Western       1.00      1.00      1.00        25\n",
      "       Thriller       0.98      0.85      0.91       184\n",
      "          Crime       1.00      0.90      0.95       143\n",
      "Science Fiction       0.99      0.93      0.96        81\n",
      "        Mystery       0.99      0.99      0.99        77\n",
      "          Music       1.00      0.82      0.90        34\n",
      "        Romance       1.00      0.85      0.92       124\n",
      "         Family       1.00      0.90      0.95        92\n",
      "            War       1.00      1.00      1.00        42\n",
      "       TV Movie       1.00      1.00      1.00         5\n",
      "\n",
      "    avg / total       0.98      0.93      0.95      2025\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.78      0.45      0.57        31\n",
      "        Fantasy       1.00      0.18      0.30        17\n",
      "      Animation       1.00      0.06      0.11        17\n",
      "          Drama       0.80      0.88      0.84       130\n",
      "         Horror       1.00      0.09      0.17        11\n",
      "         Action       0.86      0.30      0.44        40\n",
      "         Comedy       0.88      0.14      0.25        49\n",
      "        History       0.00      0.00      0.00        19\n",
      "        Western       0.00      0.00      0.00         8\n",
      "       Thriller       0.69      0.28      0.40        39\n",
      "          Crime       0.82      0.27      0.41        33\n",
      "Science Fiction       0.80      0.32      0.46        25\n",
      "        Mystery       0.00      0.00      0.00        19\n",
      "          Music       0.00      0.00      0.00         5\n",
      "        Romance       0.40      0.07      0.12        28\n",
      "         Family       0.80      0.24      0.36        17\n",
      "            War       1.00      0.26      0.42        19\n",
      "       TV Movie       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.73      0.38      0.44       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results: ', best_precision)\n",
    "print(resultsDict[best_precision]['train_classification_report'])\n",
    "print(resultsDict[best_precision]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive-Bayes-combined_bow analysis**\n",
    "\n",
    "There appears to be a an overfit occuring with this model - the train scores for precision and recall are 0.98 and 0.93, respectively, whereas the test results are much lower. This model has the best precision score out of any of the models, meaning that there are many true-positives and few false-positives. However, the recall score is much lower than many of the other models - indicating that this model has the tendency to result in false-negatives.\n",
    "\n",
    "**Genre-Specific Results for this model:**\n",
    "Using the test dataset, this model was able to identify Fantasy, Animation, Horror, and War genres with 100% accuracy. In other words - every time this model predicted one of those genres, it was correct. However, this model was somewhat \"hesitant\" to make those predictions - which is what caused the recall score to be so low. \n",
    "\n",
    "This model is best at predicting Drama movies. Although the precision score is lower than the genres listed above, it is less \"hesitant\" to make drama predictions, as evidenced by the higher recall score. This can be partially explained by the skewed dataset, which had far more drama movies than any other genre. This allowed the model to be fit on a more diverse set of words when compared to other genres.\n",
    "\n",
    "This model was unable to succesfully predict any History, Western, Mystery, or TV Movie genres. This is likely a result of the model being overfit. This claim is supported by the f1-scores in the training set of these genres, which have scores of [1.00, 1.00, 0.99, and 1.00], respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  SVC-combined_w2v_mean\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.58      0.73      0.65       137\n",
      "        Fantasy       0.38      0.68      0.49        76\n",
      "      Animation       0.89      1.00      0.94        82\n",
      "          Drama       0.90      0.86      0.88       515\n",
      "         Horror       0.93      1.00      0.96        41\n",
      "         Action       0.56      0.92      0.70       124\n",
      "         Comedy       0.59      0.91      0.71       189\n",
      "        History       0.90      1.00      0.95        54\n",
      "        Western       0.78      1.00      0.88        25\n",
      "       Thriller       0.61      0.86      0.71       184\n",
      "          Crime       0.62      0.81      0.70       143\n",
      "Science Fiction       0.89      1.00      0.94        81\n",
      "        Mystery       0.73      1.00      0.85        77\n",
      "          Music       0.92      1.00      0.96        34\n",
      "        Romance       0.70      0.98      0.82       124\n",
      "         Family       0.88      1.00      0.94        92\n",
      "            War       0.95      1.00      0.98        42\n",
      "       TV Movie       1.00      1.00      1.00         5\n",
      "\n",
      "    avg / total       0.74      0.89      0.80      2025\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.43      0.74      0.55        31\n",
      "        Fantasy       0.27      0.65      0.38        17\n",
      "      Animation       0.52      0.76      0.62        17\n",
      "          Drama       0.85      0.77      0.81       130\n",
      "         Horror       0.60      0.55      0.57        11\n",
      "         Action       0.46      0.68      0.55        40\n",
      "         Comedy       0.49      0.76      0.60        49\n",
      "        History       0.20      0.16      0.18        19\n",
      "        Western       0.80      1.00      0.89         8\n",
      "       Thriller       0.42      0.77      0.55        39\n",
      "          Crime       0.56      0.73      0.63        33\n",
      "Science Fiction       0.61      0.68      0.64        25\n",
      "        Mystery       0.28      0.37      0.32        19\n",
      "          Music       0.17      0.40      0.24         5\n",
      "        Romance       0.29      0.46      0.36        28\n",
      "         Family       0.52      0.76      0.62        17\n",
      "            War       0.70      0.84      0.76        19\n",
      "       TV Movie       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.56      0.69      0.61       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results: ', best_recall)\n",
    "print(resultsDict[best_recall]['train_classification_report'])\n",
    "print(resultsDict[best_recall]['test_classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC-combined_w2v_mean Analysis:**\n",
    "\n",
    "Unlike the **Naive-Bayes-combined_bow** model, this model does not have any perfect precision scores in the test dataset. This is because this model is less \"hesitant\" to make predictions - leading to a larger amount of false positives and thus lowering the precision score. Consequently, because it is less \"hesitant\" to make predictions, there are fewer false-negatives, which results in the higher recall score. Compared to the f1 score, this model outperforms the Naive-Bayes model. \n",
    "\n",
    "**Genre-Specific Results for this model:**\n",
    "With regard to precision, this model is most accurate at identifying Drama, Western, War, Science Fiction, and Horror. Like the Naive-Bayes model, this model also predicts 0 genres as TV Movie. This is likely caused by the few occurences of this genre in the dataset - only occuring 5 times in the training dataset and once in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Although **Naive-Bayes-combined_bow** has a better precision accuracy metric, it is being overfit - as evidenced by the perfect f1-scores in the training dataset in addition to the much lower test results. When this model *does* make predictions, the predictions tend to be accurate, but this model simply does not make many predictions. This is shown by the low recall score, as well as the 4 genres that it made 0 predictions for.\n",
    "\n",
    "**SVC-combined_w2v_mean** has a lower precision score, but has much higher recall scores and f1-scores. It is also evident that there is less of an overfit occuring.\n",
    "\n",
    "~here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Hesitation\" to make predictions\n",
    "\n",
    "The theory described above can be mathematically proven by looking at the `predict_proba` attribute of each model. In a MultiLabel classification model, the predict_proba attribute returns an array that represents the probability of that individual observation belonging to each class. For each of the probabilities that are above 0.5, the model predicts that the observation belongs to that corresponding class. Let's look at a simple example to describe this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDict[best_precision]['fitted_model'].predict(best_precision_predictor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For observation 0, the array above represents classes 0 through 17. Class 3 and class 10 have a value of 1, indicating that the model predicts this movie to be these two genres. Each remaining class that has a value of 0 is not predicted as a genre.\n",
    "\n",
    "Thus, in the cell below, we will see that every class besides 3 and 10 will have a value less than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.04719005e-02, 1.66590112e-02, 5.92908140e-04, 9.36481209e-01,\n",
       "       7.46583129e-04, 3.35582768e-02, 2.40284760e-02, 5.54701095e-04,\n",
       "       1.26084883e-04, 1.50322996e-01, 7.33628097e-01, 3.91758746e-03,\n",
       "       1.74222489e-03, 1.38138201e-02, 9.17876692e-03, 2.87533281e-02,\n",
       "       8.28736853e-04, 6.20652724e-05])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDict[best_precision]['fitted_model'].predict_proba(best_precision_predictor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have reviewed how this function works, we can mathematically prove our argument that the Naive-Bayes Model is more \"hesitant\". Since predictions are made based on probabilities, we can take the mean of probabilities for all of the observations in the test dataset to get an overall representation of how likely the model is able to make a prediction. The funtion below accomplishes this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_probability(model, predictor):\n",
    "    _, test_x = train_test_split(predictor, test_size=0.2, random_state=9001)\n",
    "    \n",
    "    return model.predict_proba(test_x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this function on our Naive-Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10182064002264195"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probability(resultsDict[best_precision]['fitted_model'], best_precision_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional step is required before we can make the comparison to our **SVC-combined_w2v_mean** model. By default, fitted SVC models do not include a predict_proba attribute. We chose to keep the default when looping through our models because it takes much longer to fit a model when this is enabled.\n",
    "\n",
    "Now that we have identified our best model, we will re-run it through the function, with making an adjustment to include the predict_proba attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall_rerun = {'model':SVC(class_weight='balanced', kernel='linear',\n",
    "                                probability=True),\n",
    "                   'params':{'C':[0.01,0.1,1.0]}}\n",
    "        \n",
    "        \n",
    "best_recall_rerun_results = evaluate_model(model = model_rerun['model'],\n",
    "                        predictors = predictorDict['combined_w2v_mean'], \n",
    "                        response = binary_tmdb,\n",
    "                        cv=True,\n",
    "                        params=model_rerun['params'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have added this attribute, we can make the comparison to our **Naive-Bayes-combined_bow** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14964159310559547"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probability(best_recall_rerun_results['fitted_model'], best_recall_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this model is ~48% more likely to make a genre prediction, compared to the **Naive-Bayes-combined_bow** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting\n",
    "\n",
    "Subsetting on the data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_source_results = {}\n",
    "\n",
    "for group in ['imdb', 'tmdb', 'combined']:\n",
    "    subset = results_df.filter(like=group, axis=0)\n",
    "\n",
    "    \n",
    "    data_source_results[group] = {'min_test_precision': subset['test_precision_score'].min(),\n",
    "                                'max_test_precision': subset['test_precision_score'].max(),\n",
    "                                'mean_test_precision':subset['test_precision_score'].mean(),\n",
    "                                'min_test_recall': subset['test_recall_score'].min(),\n",
    "                                'max_test_recall': subset['test_recall_score'].max(),\n",
    "                                'mean_test_recall':subset['test_recall_score'].mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_test_precision</th>\n",
       "      <th>max_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>min_test_recall</th>\n",
       "      <th>max_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>combined</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.725255</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.395888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imdb</th>\n",
       "      <td>0.153214</td>\n",
       "      <td>0.607838</td>\n",
       "      <td>0.328140</td>\n",
       "      <td>0.253937</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.375984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmdb</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.619958</td>\n",
       "      <td>0.345431</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.374016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          min_test_precision  max_test_precision  mean_test_precision  \\\n",
       "combined            0.166339            0.725255             0.402100   \n",
       "imdb                0.153214            0.607838             0.328140   \n",
       "tmdb                0.166339            0.619958             0.345431   \n",
       "\n",
       "          min_test_recall  max_test_recall  mean_test_recall  \n",
       "combined         0.255906         0.688976          0.395888  \n",
       "imdb             0.253937         0.685039          0.375984  \n",
       "tmdb             0.255906         0.633858          0.374016  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_order = ['min_test_precision','max_test_precision','mean_test_precision',\n",
    "                                            'min_test_recall','max_test_recall','mean_test_recall']\n",
    "\n",
    "pd.DataFrame(data_source_results).transpose()[column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EDA notebook, we discovered that the imdb plot descriptions tend to have more words than the tmdb plot description. We originally suspected that this would lead to higher accuracy metrics, since the plots are theoretically more descriptive if they are longer. However, the results above demonstrate that this is not the case. \n",
    "\n",
    "We do see that combining the plot descriptions leads to higher scores. What is particularly interesting is that the recall score does not improve as much as the precision score. In other words, combining the plot descriptions leads to considerably fewer false-positives, but has a lesser effect on preventing false-negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor_results = {}\n",
    "for group in ['bow', 'w2v', 'doc_vec']:\n",
    "    subset = results_df.filter(like=group, axis=0)\n",
    "\n",
    "    predictor_results[group] = {'min_test_precision': subset['test_precision_score'].min(),\n",
    "                                'max_test_precision': subset['test_precision_score'].max(),\n",
    "                                'mean_test_precision':subset['test_precision_score'].mean(),\n",
    "                                'min_test_recall': subset['test_recall_score'].min(),\n",
    "                                'max_test_recall': subset['test_recall_score'].max(),\n",
    "                                'mean_test_recall':subset['test_recall_score'].mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_test_precision</th>\n",
       "      <th>max_test_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>min_test_recall</th>\n",
       "      <th>max_test_recall</th>\n",
       "      <th>mean_test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bow</th>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.725255</td>\n",
       "      <td>0.466946</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.373360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_vec</th>\n",
       "      <td>0.153214</td>\n",
       "      <td>0.186744</td>\n",
       "      <td>0.169415</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.325241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v</th>\n",
       "      <td>0.178442</td>\n",
       "      <td>0.558305</td>\n",
       "      <td>0.439310</td>\n",
       "      <td>0.253937</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.447288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         min_test_precision  max_test_precision  mean_test_precision  \\\n",
       "bow                0.166339            0.725255             0.466946   \n",
       "doc_vec            0.153214            0.186744             0.169415   \n",
       "w2v                0.178442            0.558305             0.439310   \n",
       "\n",
       "         min_test_recall  max_test_recall  mean_test_recall  \n",
       "bow             0.255906         0.551181          0.373360  \n",
       "doc_vec         0.255906         0.525591          0.325241  \n",
       "w2v             0.253937         0.688976          0.447288  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(predictor_results).transpose()[column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words has the best performance with regard to both precision and recall. Word2vec comes shortly there after. Doc2vec has by far the lowest performance of the three, which is expected in considering the difference in granularity. Word2vec analyzes the plots on a word-by-word basis, whereas Doc2vec analyzes the entire plot as a single vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
